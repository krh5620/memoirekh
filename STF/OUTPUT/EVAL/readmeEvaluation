We have also developed and released a useful script to calculate macro-averages called average_of_evaluations.py. 

This script takes as input several files generated by the evaluation script (you can redirect the output) 

and generates averages for all metrics. 

For instance, you can run the following commands to calculate the macro-averaged scores between Bulgarian (BG) and German (DE):

./evaluate.py --gold BG/dev.cupt --pred BG/system.cupt --train BG/train.cupt > BG/eval.txt 
./evaluate.py --gold DE/dev.cupt --pred DE/system.cupt --train DE/train.cupt > DE/eval.txt 
./average_of_evaluations.py BG/eval.txt DE/eval.txt

Average on all languages of the Shared Task :
./average_of_evaluations.py  BG/eval.txt DE/eval.txt EL/eval.txt ES/e
val.txt EU/eval.txt FA/eval.txt FR/eval.txt HE/eval.txt HR/eval.txt HU/eval.txt IT/eval.txt PL/eval.txt PT/eval.txt RO/eval.txt SL/eval.txt TR/eval.txt > averageALL.txt



